# System Architecture â€” GEC Demo

This document describes the internal architecture and data flow of the Grammar Error Correction system.

---

## ğŸ“ High-Level Overview

The system is organized as a multi-layer pipeline:

````

Browser UI
â†“
Go HTTP Server
â†“
Native Inference Runtime (C/C++)
â†“
ONNX Runtime + SentencePiece
â†“
Model Output

```

Each layer is optimized for separation of concerns and performance.

---

## ğŸ§© Component Diagram

```

+------------------+
|  Web Frontend    |
|  (HTML / JS)     |
+--------+---------+
|
| HTTP POST
v
+------------------+
|  Go API Server   |
|  (net/http)      |
+--------+---------+
|
| CGO / FFI
v
+------------------+
| Native Runtime   |
|  (C / C++)        |
+--------+---------+
|
| ONNX API
v
+------------------+
| ONNX Runtime     |
| SentencePiece    |
+--------+---------+
|
v
+------------------+
| GEC ONNX Model   |
+------------------+

```

---

## ğŸŒ Frontend Layer

**Location:** `webpage/`

Responsibilities:

- Collect user input
- Send requests to backend
- Render marked-up output
- Display diagnostics

Technologies:

- Vanilla JavaScript
- HTML/CSS
- Fetch API

---

## ğŸ–¥ï¸ Backend API Layer

**Location:** `src/cmd/gec-server`, `src/internal/api`

Responsibilities:

- HTTP routing
- Input validation
- Request lifecycle management
- Response formatting
- Logging

The server exposes REST endpoints and manages concurrency.

---

## âš™ï¸ Native Inference Layer

**Location:** `src/native/gec_runtime`

Responsibilities:

- Load ONNX models
- Initialize ONNX Runtime sessions
- Manage memory buffers
- Tokenize input
- Run inference
- Decode outputs

Implemented in C/C++ for:

- Low-level memory control
- Reduced overhead
- Maximum inference throughput

---

## ğŸ”— Go â†” C Integration

The backend communicates with the native runtime using CGO bindings.

Key design goals:

- Minimal copying
- Deterministic memory ownership
- Explicit error propagation
- Thread-safe inference calls

---

## ğŸ§  Model Layer

**Location:** `models/GecModel`

Contents:

- Encoder/decoder ONNX graphs
- Tokenizer configuration
- SentencePiece model
- Generation config

The model is versioned and stored using Git LFS.

---

## ğŸ”„ Request Lifecycle

### 1. User Submission
User enters text in browser.

### 2. HTTP Request
Frontend sends POST `/grammar`.

### 3. Validation
Go server validates JSON payload.

### 4. Tokenization
Text is passed to native runtime and tokenized.

### 5. Inference
ONNX Runtime executes encoder/decoder graphs.

### 6. Decoding
Output tokens converted back to text.

### 7. Markup
Differences are computed and annotated.

### 8. Response
Formatted JSON is returned.

---

## ğŸ“Š Performance Considerations

### Memory

- Preloaded model sessions
- Reused inference buffers
- Avoided dynamic allocation in hot paths

### Latency

- Native tokenization
- Batched ONNX calls
- Minimal CGO overhead

### Concurrency

- Thread-safe inference sessions
- Goroutine-based request handling

---

## ğŸ³ Container Architecture

```

Docker Image
â”œâ”€â”€ Go Binary
â”œâ”€â”€ Native Runtime
â”œâ”€â”€ ONNX Runtime
â””â”€â”€ Model Files

```

Runtime configuration via environment variables:

- `LOG_LEVEL`
- `HF_TOKEN`
- `PORT`

---

## ğŸ“ˆ Observability

### Logging

- Structured logs
- Configurable verbosity
- Request timing

### Diagnostics

- Startup validation
- Model presence checks
- Error codes

---

## ğŸ”’ Security Architecture

- Secrets injected at runtime
- No credentials baked into images
- Isolated container environment
- Limited filesystem access

---

## â™»ï¸ Extensibility

Designed to support:

- Model swapping
- Multi-model routing
- GPU acceleration
- Batch inference
- Streaming inference

Minimal changes required for new models.

---

## ğŸš§ Known Limitations

- Single-model deployment
- No horizontal scaling
- No authentication layer
- CPU-only by default

These are intentional for portfolio scope.

---

## ğŸ“š Related Documents

- `README.md`
- `MODEL_CARD.md`
- `API.md`
- `THIRD_PARTY_NOTICES.md`